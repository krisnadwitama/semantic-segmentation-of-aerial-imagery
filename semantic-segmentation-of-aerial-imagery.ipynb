{"metadata":{"colab":{"provenance":[],"collapsed_sections":["zJLmdb7TARPS","ahhqqjHzAU3m","1Q1CKcZTAYUM","lcmfbqWiOfJP","L-40Q7aSAdEm","b2o0fpMT-Oq1","d0-icA0wSR_P","jZs4cmGHXQkv","tVrKq1hWAj4C","vGaMUCNhAhtW"]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### Import Libraries","metadata":{"id":"ahhqqjHzAU3m"}},{"cell_type":"code","source":"# pip install segmentation-models","metadata":{"id":"vf723TVx5w0K","execution":{"iopub.status.busy":"2022-09-30T06:12:54.062828Z","iopub.execute_input":"2022-09-30T06:12:54.063252Z","iopub.status.idle":"2022-09-30T06:12:54.067735Z","shell.execute_reply.started":"2022-09-30T06:12:54.063207Z","shell.execute_reply":"2022-09-30T06:12:54.066732Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# pip install -q git+https://github.com/tensorflow/examples.git","metadata":{"execution":{"iopub.status.busy":"2022-09-30T06:12:54.069565Z","iopub.execute_input":"2022-09-30T06:12:54.070479Z","iopub.status.idle":"2022-09-30T06:12:54.079135Z","shell.execute_reply.started":"2022-09-30T06:12:54.070432Z","shell.execute_reply":"2022-09-30T06:12:54.078259Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# pip install 'git+https://github.com/facebookresearch/detectron2.git'","metadata":{"id":"2d8cyc04m2pw","execution":{"iopub.status.busy":"2022-09-30T06:12:54.080682Z","iopub.execute_input":"2022-09-30T06:12:54.081494Z","iopub.status.idle":"2022-09-30T06:12:54.088164Z","shell.execute_reply.started":"2022-09-30T06:12:54.081458Z","shell.execute_reply":"2022-09-30T06:12:54.087221Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import sklearn\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport math\n\n# import detectron2\n# import segmentation_models as sm\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.utils import plot_model\n\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPooling2D, Dropout, Conv2DTranspose, concatenate, ZeroPadding2D, Concatenate\nfrom tensorflow.keras import Model, Input\nfrom keras import backend as K\n\nimport cv2","metadata":{"id":"rPRKkkDPlUAV","colab":{"base_uri":"https://localhost:8080/"},"outputId":"896bbef5-8563-4645-89ed-1ac68fd6a99d","execution":{"iopub.status.busy":"2022-09-30T06:12:54.090609Z","iopub.execute_input":"2022-09-30T06:12:54.091405Z","iopub.status.idle":"2022-09-30T06:13:00.305669Z","shell.execute_reply.started":"2022-09-30T06:12:54.091370Z","shell.execute_reply":"2022-09-30T06:13:00.304659Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"#### Data Preparation","metadata":{"id":"L-40Q7aSAdEm"}},{"cell_type":"code","source":"# Mapping color from kaggle (in BGR format)\n# Building: #3C1098 --> Use building only = [152, 16, 60]\n# Land (unpaved area): #8429F6\n# Road: #6EC1E4\n# Vegetation: #FEDD3A\n# Water: #E2A929\n# Unlabeled: #9B9B9B\n\n# Load data\ndataset_feature = []\ndataset_label = []\ndataset_label2 = []\n\nfor i in range(1, 9):\n  for j in range(1, 37):\n    if j < 10:\n        path = \"../input/aerial-building-only-augmented/Semantic segmentation dataset augmented/Tile \" + str(i) + \"/images/image_part_00\" + str(j) + \".jpg\"\n    else:\n        path = \"../input/aerial-building-only-augmented/Semantic segmentation dataset augmented/Tile \" + str(i) + \"/images/image_part_0\" + str(j) + \".jpg\"\n    \n    img = cv2.imread(path)\n    img2 = cv2.resize(img, (512, 512), interpolation=cv2.INTER_NEAREST)\n\n    dataset_feature.append(img2)\n\ndataset_feature = np.array(dataset_feature)\ndataset_feature = dataset_feature.astype('float32')\n\ncnt0 = 0\ncnt1 = 1\n\nfor i in range(1, 9):\n  for j in range(1, 37):\n    if j < 10:\n        path = \"../input/aerial-building-only-augmented/Semantic segmentation dataset augmented/Tile \" + str(i) + \"/masks/image_part_00\" + str(j) + \".png\"\n    else:\n        path = \"../input/aerial-building-only-augmented/Semantic segmentation dataset augmented/Tile \" + str(i) + \"/masks/image_part_0\" + str(j) + \".png\"\n    \n    img = cv2.imread(path)\n    img2 = cv2.resize(img, (512, 512), interpolation=cv2.INTER_NEAREST)\n    \n    dataset_label.append(img2)\n\n    # Convert the color value\n    temp = []\n    for ii in range(512):\n      temp2 = []\n      for jj in range(512):\n        if img2[ii][jj].tolist() == [152, 16, 60]:\n          temp2.append(1)\n          cnt1 += 1\n        else:\n          temp2.append(0)\n          cnt0 += 1\n      temp.append(temp2)\n    dataset_label2.append(temp)\n\nprint(\"Counter Label 0:\", cnt0)\nprint(\"Counter Label 1:\", cnt1)\n\n# dataset_label = np.array(dataset_label)\ndataset_label2 = np.array(dataset_label2)\ndataset_label2 = dataset_label2.astype('float32')\n# dataset_label = dataset_label / 255.0\n# dataset_label = dataset_label.astype('float32')\n\nx_train_val, x_testing, y_train_val, y_testing = train_test_split(dataset_feature, dataset_label2)\nx_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val)\nprint(x_train_val.shape)\nprint(x_testing.shape)\nprint(y_train_val.shape)\nprint(y_testing.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Wyw1oX53wQa","outputId":"d759482b-ea28-44b9-8a7d-d121b90614f4","execution":{"iopub.status.busy":"2022-09-30T06:13:00.307254Z","iopub.execute_input":"2022-09-30T06:13:00.307897Z","iopub.status.idle":"2022-09-30T06:14:24.914866Z","shell.execute_reply.started":"2022-09-30T06:13:00.307858Z","shell.execute_reply":"2022-09-30T06:14:24.913763Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Counter Label 0: 67553001\nCounter Label 1: 7944472\n(216, 512, 512, 3)\n(72, 512, 512, 3)\n(216, 512, 512)\n(72, 512, 512)\n","output_type":"stream"}]},{"cell_type":"code","source":"arr_label = []\nimage_label = []\n\nsz = x_testing.shape[0]\nfor i in range(sz):\n  temp_label = []\n  temp_image_label = []\n  for j in range(512):\n    temp_label2 = []\n    temp_image_label2 = []\n    for k in range(512):\n      label = y_testing[i][j][k]\n      if label == 1:\n        label_image = [0, 0, 0]\n      else:\n        label_image = [255, 255, 255]\n      temp_label2.append(label)\n      temp_image_label2.append(label_image)\n    temp_label.append(temp_label2)\n    temp_image_label.append(temp_image_label2)\n  arr_label.append(temp_label)\n  image_label.append(temp_image_label)\n\narr_label = np.array(arr_label)\nimage_label = np.array(image_label)","metadata":{"execution":{"iopub.status.busy":"2022-09-30T06:14:24.916256Z","iopub.execute_input":"2022-09-30T06:14:24.917140Z","iopub.status.idle":"2022-09-30T06:15:34.742160Z","shell.execute_reply.started":"2022-09-30T06:14:24.917110Z","shell.execute_reply":"2022-09-30T06:15:34.741092Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"#### Metric Evaluation and Loss Function","metadata":{"id":"b2o0fpMT-Oq1"}},{"cell_type":"code","source":"def recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n\n# Class Weight Loss for Imbalance Dataset\ndef create_weighted_binary_crossentropy(zero_weight, one_weight):\n    def weighted_binary_crossentropy(y_true, y_pred):\n        # Calculate the binary crossentropy\n        b_ce = K.binary_crossentropy(y_true, y_pred)\n\n        # Apply the weights\n        weight_vector = y_true * one_weight + (1.0 - y_true) * zero_weight\n        weighted_b_ce = weight_vector * b_ce\n\n        # Return the mean error\n        return K.mean(weighted_b_ce)\n\n    return weighted_binary_crossentropy","metadata":{"id":"Hn3Ji99j-bpf","execution":{"iopub.status.busy":"2022-09-30T06:15:34.745235Z","iopub.execute_input":"2022-09-30T06:15:34.745634Z","iopub.status.idle":"2022-09-30T06:15:34.755473Z","shell.execute_reply.started":"2022-09-30T06:15:34.745597Z","shell.execute_reply":"2022-09-30T06:15:34.754275Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"#### UNET Model","metadata":{"id":"1Q1CKcZTAYUM"}},{"cell_type":"code","source":"def conv2d_block(input_tensor, n_filters, kernel_size = 3, batchnorm = True):\n    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n    # first layer\n    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n    if batchnorm:\n        x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    \n    # second layer\n    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n    if batchnorm:\n        x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    \n    return x\n  \ndef get_unet(input_img, n_filters = 16, dropout = 0.1, batchnorm = True):\n    # Contracting Path\n    c1 = conv2d_block(input_img, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n    p1 = MaxPooling2D((2, 2))(c1)\n    p1 = Dropout(dropout)(p1)\n    \n    c2 = conv2d_block(p1, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n    p2 = MaxPooling2D((2, 2))(c2)\n    p2 = Dropout(dropout)(p2)\n    \n    c3 = conv2d_block(p2, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n    p3 = MaxPooling2D((2, 2))(c3)\n    p3 = Dropout(dropout)(p3)\n    \n    c4 = conv2d_block(p3, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n    p4 = MaxPooling2D((2, 2))(c4)\n    p4 = Dropout(dropout)(p4)\n    \n    c5 = conv2d_block(p4, n_filters = n_filters * 16, kernel_size = 3, batchnorm = batchnorm)\n    \n    # Expansive Path\n    u6 = Conv2DTranspose(n_filters * 8, (3, 3), strides = (2, 2), padding = 'same')(c5)\n    u6 = concatenate([u6, c4])\n    u6 = Dropout(dropout)(u6)\n    c6 = conv2d_block(u6, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n    \n    u7 = Conv2DTranspose(n_filters * 4, (3, 3), strides = (2, 2), padding = 'same')(c6)\n    u7 = concatenate([u7, c3])\n    u7 = Dropout(dropout)(u7)\n    c7 = conv2d_block(u7, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n    \n    u8 = Conv2DTranspose(n_filters * 2, (3, 3), strides = (2, 2), padding = 'same')(c7)\n    u8 = concatenate([u8, c2])\n    u8 = Dropout(dropout)(u8)\n    c8 = conv2d_block(u8, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n    \n    u9 = Conv2DTranspose(n_filters * 1, (3, 3), strides = (2, 2), padding = 'same')(c8)\n    u9 = concatenate([u9, c1])\n    u9 = Dropout(dropout)(u9)\n    c9 = conv2d_block(u9, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n    \n    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n    model = Model(inputs=[input_img], outputs=[outputs])\n    return model","metadata":{"id":"vkh8acc53bRD","execution":{"iopub.status.busy":"2022-09-30T06:15:34.757171Z","iopub.execute_input":"2022-09-30T06:15:34.757570Z","iopub.status.idle":"2022-09-30T06:15:34.776621Z","shell.execute_reply.started":"2022-09-30T06:15:34.757534Z","shell.execute_reply":"2022-09-30T06:15:34.775720Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"#### InceptionResNetV2-UNet Model (Transfer Learning)","metadata":{"id":"lcmfbqWiOfJP"}},{"cell_type":"code","source":"from tensorflow.keras.applications import InceptionResNetV2\n\ndef conv_block(input, num_filters):\n    x = Conv2D(num_filters, 3, padding=\"same\")(input)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n\n    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n\n    return x\n\ndef decoder_block(input, skip_features, num_filters):\n    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n    x = Concatenate()([x, skip_features])\n    x = conv_block(x, num_filters)\n    return x\n\ndef build_inception_resnetv2_unet(input_shape):\n    \"\"\" Input \"\"\"\n    inputs = Input(input_shape)\n\n    \"\"\" Pre-trained InceptionResNetV2 Model \"\"\"\n    encoder = InceptionResNetV2(include_top=False, weights=\"imagenet\", input_tensor=inputs)\n\n    \"\"\" Encoder \"\"\"\n    s1 = encoder.get_layer(\"input_1\").output           ## (512 x 512)\n\n    s2 = encoder.get_layer(\"activation\").output        ## (255 x 255)\n    s2 = ZeroPadding2D(( (1, 0), (1, 0) ))(s2)         ## (256 x 256)\n\n    s3 = encoder.get_layer(\"activation_3\").output      ## (126 x 126)\n    s3 = ZeroPadding2D((1, 1))(s3)                     ## (128 x 128)\n\n    s4 = encoder.get_layer(\"activation_74\").output      ## (61 x 61)\n    s4 = ZeroPadding2D(( (2, 1),(2, 1) ))(s4)           ## (64 x 64)\n\n    \"\"\" Bridge \"\"\"\n    b1 = encoder.get_layer(\"activation_161\").output     ## (30 x 30)\n    b1 = ZeroPadding2D((1, 1))(b1)                      ## (32 x 32)\n\n    \"\"\" Decoder \"\"\"\n    d1 = decoder_block(b1, s4, 512)                     ## (64 x 64)\n    d2 = decoder_block(d1, s3, 256)                     ## (128 x 128)\n    d3 = decoder_block(d2, s2, 128)                     ## (256 x 256)\n    d4 = decoder_block(d3, s1, 64)                      ## (512 x 512)\n    \n    \"\"\" Output \"\"\"\n    # dropout = Dropout(0.3)(d4)\n    dropout = Dropout(0.1)(d4)\n    outputs = Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(dropout)\n\n    model = Model(inputs, outputs, name=\"InceptionResNetV2-UNet\")\n    return model","metadata":{"id":"2qQAAiPWOi7e","execution":{"iopub.status.busy":"2022-09-30T06:15:34.778185Z","iopub.execute_input":"2022-09-30T06:15:34.778542Z","iopub.status.idle":"2022-09-30T06:15:34.792528Z","shell.execute_reply.started":"2022-09-30T06:15:34.778508Z","shell.execute_reply":"2022-09-30T06:15:34.791600Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"keras.optimizers.Adam().learning_rate","metadata":{"execution":{"iopub.status.busy":"2022-09-30T06:15:34.793873Z","iopub.execute_input":"2022-09-30T06:15:34.794388Z","iopub.status.idle":"2022-09-30T06:15:37.499408Z","shell.execute_reply.started":"2022-09-30T06:15:34.794330Z","shell.execute_reply":"2022-09-30T06:15:37.498302Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"2022-09-30 06:15:34.877953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-30 06:15:34.992747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-30 06:15:34.993623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-30 06:15:34.995481: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-09-30 06:15:34.995817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-30 06:15:34.996777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-30 06:15:34.997735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-30 06:15:37.131029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-30 06:15:37.131856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-30 06:15:37.132534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-30 06:15:37.133157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15401 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Semantic Segmentation","metadata":{"id":"OOHW-3-fPQL4"}},{"cell_type":"code","source":"K.clear_session()\n\n# Define the model architecture\ninput_img = Input((512, 512, 3), name='Input')\n\n# model = get_unet(input_img, n_filters=16, dropout=0.1)\nmodel = build_inception_resnetv2_unet(input_shape=(512, 512, 3))\n\n# Compile the model\n# optimizer = keras.optimizers.Adam(learning_rate=0.01)\noptimizer = keras.optimizers.Adam()\n# optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n# optimizer = keras.optimizers.RMSprop()\n# optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n# optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\n# optimizer = keras.optimizers.SGD(learning_rate=0.0001, momentum=0.9)\n\n# Class Weight for Imbalance Dataset\nclass_weight_0 = (cnt0 + cnt1) / cnt0\nclass_weight_1 = (cnt0 + cnt1) / cnt1\nmodel.compile(optimizer=optimizer,\n#               loss=create_weighted_binary_crossentropy(class_weight_0, class_weight_1),\n              loss = 'binary_crossentropy',\n              metrics=[f1_m, 'accuracy'])\n\nhistory = model.fit(\n  x=x_train,\n  y=y_train,\n  batch_size=8,\n#   epochs=20,\n  epochs=50,\n  validation_data=(x_val, y_val),\n  verbose=1\n)\n\n# Generate generalization metrics\nscores_training = model.evaluate(x_train, y_train, verbose=0)\nprint('----------------------------------------------------------------------------')\nprint(\"Training Data\")\nprint(\"F1-Score:\", scores_training[1] * 100)\nprint(\"Accuracy:\", scores_training[2] * 100)\n\nscores_validation = model.evaluate(x_val, y_val, verbose=0)\nprint('----------------------------------------------------------------------------')\nprint(\"Validation Data\")\nprint(\"F1-Score:\", scores_validation[1] * 100)\nprint(\"Accuracy:\", scores_validation[2] * 100)\nprint('----------------------------------------------------------------------------')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CG3VZKB8RI4E","outputId":"7036aa91-74ec-4a72-e659-1c3264c47387","execution":{"iopub.status.busy":"2022-09-30T06:59:46.602436Z","iopub.execute_input":"2022-09-30T06:59:46.603054Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/50\n21/21 [==============================] - 37s 1s/step - loss: 0.5285 - f1_m: 0.4452 - accuracy: 0.8002 - val_loss: 13.5806 - val_f1_m: 0.1942 - val_accuracy: 0.1082\nEpoch 2/50\n21/21 [==============================] - 20s 974ms/step - loss: 0.2709 - f1_m: 0.5155 - accuracy: 0.9283 - val_loss: 12.5767 - val_f1_m: 0.1960 - val_accuracy: 0.1186\nEpoch 3/50\n21/21 [==============================] - 20s 968ms/step - loss: 0.2388 - f1_m: 0.5091 - accuracy: 0.9253 - val_loss: 12.6916 - val_f1_m: 0.1976 - val_accuracy: 0.1286\nEpoch 4/50\n21/21 [==============================] - 20s 971ms/step - loss: 0.2047 - f1_m: 0.5704 - accuracy: 0.9331 - val_loss: 12.0191 - val_f1_m: 0.1975 - val_accuracy: 0.1269\nEpoch 5/50\n21/21 [==============================] - 20s 973ms/step - loss: 0.1797 - f1_m: 0.6597 - accuracy: 0.9386 - val_loss: 8.2242 - val_f1_m: 0.2073 - val_accuracy: 0.1824\nEpoch 6/50\n21/21 [==============================] - 20s 974ms/step - loss: 0.1589 - f1_m: 0.7025 - accuracy: 0.9423 - val_loss: 1.6320 - val_f1_m: 0.2672 - val_accuracy: 0.4337\nEpoch 7/50\n21/21 [==============================] - 20s 971ms/step - loss: 0.1435 - f1_m: 0.7014 - accuracy: 0.9474 - val_loss: 1.3582 - val_f1_m: 0.3226 - val_accuracy: 0.5749\nEpoch 8/50\n21/21 [==============================] - 20s 972ms/step - loss: 0.1315 - f1_m: 0.7374 - accuracy: 0.9518 - val_loss: 0.2367 - val_f1_m: 0.6433 - val_accuracy: 0.9160\nEpoch 9/50\n21/21 [==============================] - 20s 971ms/step - loss: 0.1359 - f1_m: 0.7160 - accuracy: 0.9475 - val_loss: 0.3520 - val_f1_m: 0.5869 - val_accuracy: 0.8725\nEpoch 10/50\n21/21 [==============================] - 20s 974ms/step - loss: 0.1270 - f1_m: 0.7407 - accuracy: 0.9520 - val_loss: 0.2298 - val_f1_m: 0.5305 - val_accuracy: 0.9197\nEpoch 11/50\n21/21 [==============================] - 20s 975ms/step - loss: 0.1131 - f1_m: 0.7833 - accuracy: 0.9582 - val_loss: 0.2607 - val_f1_m: 0.2343 - val_accuracy: 0.9031\nEpoch 12/50\n21/21 [==============================] - 20s 971ms/step - loss: 0.1357 - f1_m: 0.7119 - accuracy: 0.9502 - val_loss: 0.2149 - val_f1_m: 0.6412 - val_accuracy: 0.9130\nEpoch 13/50\n21/21 [==============================] - 20s 970ms/step - loss: 0.1135 - f1_m: 0.7885 - accuracy: 0.9567 - val_loss: 0.1827 - val_f1_m: 0.6457 - val_accuracy: 0.9271\nEpoch 14/50\n21/21 [==============================] - 20s 971ms/step - loss: 0.0973 - f1_m: 0.7987 - accuracy: 0.9639 - val_loss: 0.1447 - val_f1_m: 0.6890 - val_accuracy: 0.9416\nEpoch 15/50\n21/21 [==============================] - 20s 973ms/step - loss: 0.0904 - f1_m: 0.8232 - accuracy: 0.9659 - val_loss: 0.1679 - val_f1_m: 0.6735 - val_accuracy: 0.9348\nEpoch 16/50\n21/21 [==============================] - 20s 970ms/step - loss: 0.0851 - f1_m: 0.8311 - accuracy: 0.9670 - val_loss: 0.1478 - val_f1_m: 0.7090 - val_accuracy: 0.9343\nEpoch 17/50\n21/21 [==============================] - 20s 970ms/step - loss: 0.0827 - f1_m: 0.8327 - accuracy: 0.9677 - val_loss: 0.1099 - val_f1_m: 0.7658 - val_accuracy: 0.9543\nEpoch 18/50\n21/21 [==============================] - 20s 972ms/step - loss: 0.0736 - f1_m: 0.8576 - accuracy: 0.9722 - val_loss: 0.1172 - val_f1_m: 0.7324 - val_accuracy: 0.9546\nEpoch 19/50\n21/21 [==============================] - 20s 971ms/step - loss: 0.0745 - f1_m: 0.8451 - accuracy: 0.9715 - val_loss: 0.1171 - val_f1_m: 0.7704 - val_accuracy: 0.9522\nEpoch 20/50\n21/21 [==============================] - 20s 972ms/step - loss: 0.0715 - f1_m: 0.8525 - accuracy: 0.9724 - val_loss: 0.1131 - val_f1_m: 0.7620 - val_accuracy: 0.9535\nEpoch 21/50\n21/21 [==============================] - 20s 974ms/step - loss: 0.0677 - f1_m: 0.8620 - accuracy: 0.9740 - val_loss: 0.1177 - val_f1_m: 0.7689 - val_accuracy: 0.9517\nEpoch 22/50\n21/21 [==============================] - 20s 978ms/step - loss: 0.0683 - f1_m: 0.8636 - accuracy: 0.9734 - val_loss: 0.1292 - val_f1_m: 0.7698 - val_accuracy: 0.9502\nEpoch 23/50\n21/21 [==============================] - 20s 973ms/step - loss: 0.0642 - f1_m: 0.8486 - accuracy: 0.9750 - val_loss: 0.1154 - val_f1_m: 0.7763 - val_accuracy: 0.9547\nEpoch 24/50\n21/21 [==============================] - 20s 979ms/step - loss: 0.0744 - f1_m: 0.8276 - accuracy: 0.9710 - val_loss: 0.1514 - val_f1_m: 0.7066 - val_accuracy: 0.9362\nEpoch 25/50\n21/21 [==============================] - 20s 973ms/step - loss: 0.0757 - f1_m: 0.8375 - accuracy: 0.9709 - val_loss: 0.1534 - val_f1_m: 0.6926 - val_accuracy: 0.9470\nEpoch 26/50\n21/21 [==============================] - 20s 973ms/step - loss: 0.0681 - f1_m: 0.8552 - accuracy: 0.9726 - val_loss: 0.1181 - val_f1_m: 0.7592 - val_accuracy: 0.9557\nEpoch 27/50\n21/21 [==============================] - 20s 977ms/step - loss: 0.0695 - f1_m: 0.8335 - accuracy: 0.9717 - val_loss: 0.1392 - val_f1_m: 0.7430 - val_accuracy: 0.9533\nEpoch 28/50\n21/21 [==============================] - 20s 973ms/step - loss: 0.0585 - f1_m: 0.8775 - accuracy: 0.9772 - val_loss: 0.1324 - val_f1_m: 0.7250 - val_accuracy: 0.9547\nEpoch 29/50\n21/21 [==============================] - 20s 974ms/step - loss: 0.0554 - f1_m: 0.8757 - accuracy: 0.9784 - val_loss: 0.1122 - val_f1_m: 0.7644 - val_accuracy: 0.9588\nEpoch 30/50\n21/21 [==============================] - 20s 971ms/step - loss: 0.0581 - f1_m: 0.8452 - accuracy: 0.9779 - val_loss: 0.1235 - val_f1_m: 0.7688 - val_accuracy: 0.9536\nEpoch 31/50\n10/21 [=============>................] - ETA: 9s - loss: 0.1529 - f1_m: 0.6469 - accuracy: 0.9383 ","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Prediction Using Model and Visualize","metadata":{"id":"LtiRcekgEGCi"}},{"cell_type":"code","source":"pred = model.predict(x_testing)\narr_hasil = []\nimage_hasil = []\n\nsz = x_testing.shape[0]\nfor i in range(sz):\n  temp_hasil = []\n  temp_image = []\n  for j in range(512):\n    temp_hasil2 = []\n    temp_image2 = []\n    for k in range(512):\n      if pred[i][j][k] >= 0.5:\n        hasil = 1\n        image = [0, 0, 0]\n      else:\n        hasil = 0\n        image = [255, 255, 255]\n      temp_hasil2.append(hasil)\n      temp_image2.append(image)\n    temp_hasil.append(temp_hasil2)\n    temp_image.append(temp_image2)\n  arr_hasil.append(temp_hasil)\n  image_hasil.append(temp_image)\n\narr_hasil = np.array(arr_hasil)\nimage_hasil = np.array(image_hasil)\n\nprint(\"Testing Data F1-Score: \", end=\"\")\nprint(sklearn.metrics.f1_score(arr_label.flatten(), arr_hasil.flatten(), average=\"macro\") * 100)\nx_testing = x_testing.astype('int32')","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ROCqQSwM66rD","outputId":"8d256dcd-c475-47d9-8d3f-c94980e07e03","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_concat = np.concatenate((x_testing[17], x_testing[27], x_testing[35]), axis=1)\nlabel_concat = np.concatenate((image_label[17], image_label[27], image_label[35]), axis=1)\nhasil_concat = np.concatenate((image_hasil[17], image_hasil[27], image_hasil[35]), axis=1)\n\nprint(\"Input\")\nplt.figure(figsize=(15, 15))\nplt.imshow(input_concat)\nplt.axis(\"off\")\nplt.savefig('input.png')\nplt.show()\nprint(\"Label\")\nplt.figure(figsize=(15, 15))\nplt.imshow(label_concat)\nplt.axis(\"off\")\nplt.savefig('label.png')\nplt.show()\nprint(\"Hasil/Output\")\nplt.figure(figsize=(15, 15))\nplt.imshow(hasil_concat)\nplt.axis(\"off\")\nplt.savefig('hasil.png')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Transfer Learning Practice","metadata":{"id":"tVrKq1hWAj4C"}},{"cell_type":"code","source":"# K.clear_session()\n\n# base_model = keras.applications.Xception(weights='imagenet', input_shape=(512, 512, 3), include_top=False)\n# base_model.trainable = False\n\n# x = base_model.output\n# # x = keras.layers.GlobalAveragePooling2D()(x)\n# x = pix2pix.upsample(512, 3)(x)\n# x = pix2pix.upsample(256, 3)(x)\n# x = pix2pix.upsample(128, 3)(x)\n# x = pix2pix.upsample(64, 3)(x)\n# x = pix2pix.upsample(32, 3)(x)\n# outputs = keras.layers.Conv2D(1, 1, activation='sigmoid', padding='same')(x)\n# model = keras.Model(base_model.input, outputs)\n\n# print(model.summary)\n\n# # Compile the model\n# optimizer = keras.optimizers.Adam()\n# # optimizer = keras.optimizers.Adam(learning_rate=0.001)\n# # optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n\n# # Class Weight for Imbalance Dataset\n# # class_weight_0 = (cnt0 + cnt1) / cnt0\n# # class_weight_1 = (cnt0 + cnt1) / cnt1\n# model.compile(optimizer=optimizer,\n#               loss = 'binary_crossentropy',\n#               metrics=[f1_m, 'accuracy'])\n\n# # y_train2 = y_train.reshape(y_train.shape[0], y_train.shape[1] * y_train.shape[2])\n# # y_val2 = y_val.reshape(y_val.shape[0], y_val.shape[1] * y_val.shape[2])\n\n# history = model.fit(\n#   x=x_train,\n#   y=y_train,\n#   batch_size=8,\n#   epochs=50,\n#   validation_data=(x_val, y_val),\n#   verbose=1\n# )\n\n# # Generate generalization metrics\n# scores_training = model.evaluate(x_train, y_train, verbose=0)\n# print('----------------------------------------------------------------------------')\n# print(\"Training Data\")\n# print(\"F1-Score:\", scores_training[1] * 100)\n# print(\"Accuracy:\", scores_training[2] * 100)\n\n# scores_validation = model.evaluate(x_val, y_val, verbose=0)\n# print('----------------------------------------------------------------------------')\n# print(\"Validation Data\")\n# print(\"F1-Score:\", scores_validation[1] * 100)\n# print(\"Accuracy:\", scores_validation[2] * 100)\n# print('----------------------------------------------------------------------------')","metadata":{"id":"J53E_ZcJnkzM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pred = model.predict(x_testing)\n# # pred = pred.reshape(y_testing.shape[0], y_testing.shape[1], y_testing.shape[2])\n\n# arr_hasil = []\n# image_hasil = []\n# for i in range(16):\n#   temp_hasil = []\n#   temp_image = []\n#   for j in range(512):\n#     temp_hasil2 = []\n#     temp_image2 = []\n#     for k in range(512):\n#       if pred[i][j][k] >= 0.5:\n#         hasil = 1\n#         image = [0, 0, 0]\n#       else:\n#         hasil = 0\n#         image = [255, 255, 255]\n#       temp_hasil2.append(hasil)\n#       temp_image2.append(image)\n#     temp_hasil.append(temp_hasil2)\n#     temp_image.append(temp_image2)\n#   arr_hasil.append(temp_hasil)\n#   image_hasil.append(temp_image)\n\n# arr_hasil = np.array(arr_hasil)\n# image_hasil = np.array(image_hasil)\n\n# print(\"Testing Data F1-Score: \", end=\"\")\n# print(sklearn.metrics.f1_score(arr_label.flatten(), arr_hasil.flatten(), average=\"macro\") * 100)\n# x_testing = x_testing.astype('int32')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# input_concat = np.concatenate((x_testing[6], x_testing[9], x_testing[15]), axis=1)\n# label_concat = np.concatenate((image_label[6], image_label[9], image_label[15]), axis=1)\n# hasil_concat = np.concatenate((image_hasil[6], image_hasil[9], image_hasil[15]), axis=1)\n\n# print(\"Input\")\n# plt.figure(figsize=(15,15))\n# plt.imshow(input_concat)\n# plt.axis(\"off\")\n# plt.savefig('input.png')\n# plt.show()\n# print(\"Label\")\n# plt.figure(figsize=(15,15))\n# plt.imshow(label_concat)\n# plt.axis(\"off\")\n# plt.savefig('label.png')\n# plt.show()\n# print(\"Hasil/Output\")\n# plt.figure(figsize=(15,15))\n# plt.imshow(hasil_concat)\n# plt.axis(\"off\")\n# plt.savefig('hasil.png')\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}